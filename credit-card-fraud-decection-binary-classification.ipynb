{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":23498,"sourceType":"datasetVersion","datasetId":310}],"dockerImageVersionId":30698,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-06-10T09:16:08.584079Z","iopub.execute_input":"2024-06-10T09:16:08.584515Z","iopub.status.idle":"2024-06-10T09:16:09.031647Z","shell.execute_reply.started":"2024-06-10T09:16:08.584483Z","shell.execute_reply":"2024-06-10T09:16:09.030407Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Introduction\n![](https://cdn.pixabay.com/photo/2022/01/09/17/08/phishing-6926470_1280.png)\n\nHello everyone,\n\nI used the \"creditcardfraud\" data set in this notebook. I have developed a model that decides whether the transaction is fraud or a legit transaction using the Logistic Regression model, which is the Binary Classification methods.\n\nWhile reviewing our data set, I realized that the data set [belonged](https://www.kaggle.com/datasets/mlg-ulb/creditcardfraud) to a bank from past competitions and that the data was standardized and shared with us.\n\nOn EDA step I saw that there was no missing (NAN) data in our data. And it helped to me for less work. Later, when I looked at the distribution of dependent variables in the data set, I realize that the data was biased.\n\nDuring the modeling phase, first eliminated this bias with the Under-sampling method (there are many other different models, I have added them below(+ Plus)). Then, I divided our data into dependent and independent variables and divided it as a test-train set. Finally, I created our model and moved on to the evaluation phase.\n\nDuring the evaluation phase, I tried to use all evaluation metrics used in Classification models and tried to be as descriptive as I could.\n\nPlease keep in mind that I am still learning, so if you spot any incorrect explanations or anything, please let me know. Enjoy your journey, as well.\n\nThank you for reading this far.\n\n-------------------\n\nHerkese merhaba,\n\nBu notebook'umda \"creditcardfraud\" veri setini kullandım. Binary Classification yöntemlerinden birisi olan Logistic Regresyon modeli ile gerçekleşen işlemlerin dolandırıcılık mı yoksa gerçek bir işlem mi olduğuna karar veren bir model geliştirdim. \n\nVeri setimizi incelerken veri setinin geçmiş yarışmalardan bir bankaya ait olduğunu ve verinin Standartize edilerek bizlerle paylaşıldığını farkettim.\n\nEDA adımlarımızda verimizde hiç eksik(NAN) veri olmadığın gördüm ve bu bizim işlerimizi biraz hafifletmişti. Daha sonra Veri setinde bağımlı değişkenlerin dağılımını incelediğimizde verinin yanlı olduğunu gördüm.\n\nModelleme aşamasında ilk olarak bu yanlılığı Under-sampling metodu ile giderdik(daha bir çok farklı model var bunları da aşağıya ekledim(+ Plus)).Daha sonra verimizi bağımlı-bağımsız değişkenler olarak ayırıp test-train seti olarak ayırdım. Son olarak modelimizi oluşturdum ve değerlendirme aşamasına geçtim.\n\nDeğerlendirme aşamasında Classification modellerinde kullanılan tüm değerlendirme metriklerini kullanmaya çalıştım ve elimden geldiğince açıklayıcı olmaya çalıştım.\n\nBuraya kadar okuduğunuz için teşekkür ederim.\n","metadata":{}},{"cell_type":"markdown","source":"# + Plus\nHandling Methods with Imbalanced Data For a Classification Problem\n\n* [5 Techniques to Handle Imbalanced Data For a Classification Problem](https://www.analyticsvidhya.com/blog/2021/06/5-techniques-to-handle-imbalanced-data-for-a-classification-problem/)\n    1. Choose Proper Evaluation Metric\n    1. Resampling (Oversampling and Undersampling)\n    1. SMOTE\n    1. BalancedBaggingClassifier\n    1. Threshold Moving","metadata":{}},{"cell_type":"markdown","source":"# Content\n* [<font size=4>EDA</font>](#1)\n     * [Adding important libraries](#1.1)\n     * [First review to data ](#1.2)\n     * [NAN values](#1.3)\n     * [Distribution of Legit-Fraud classes](#1.4)\n* [<font size=4>Modelling</font>](#2)\n     * [Under-Sampling for Unbalanced dataset ](#2.1)\n     * [Creating dependent-independent variables](#2.2)\n     * [Splitting Test-Train Dataset](#2.3)\n     * [Model Instance and Fitting](#2.4)\n* [<font size=4>Model Evaluation</font>](#3)    \n     * [Evaluating with Accuracy score ](#3.1)\n     * [Evaluating with Classification Evaluation Metrics](#3.2)\n     * [Evaluating with ROC curve and AUC score](#3.3)\n         * [Plotting ROC Curve](#3.3.1)\n         * [Calculating AUC score](#3.3.2)\n* [<font size=4>Conclusion</font>](#4)","metadata":{}},{"cell_type":"markdown","source":"","metadata":{}},{"cell_type":"markdown","source":"# EDA <a id=\"1\"></a>","metadata":{}},{"cell_type":"markdown","source":"## Adding important libraries <a id=\"1.1\"></a>","metadata":{}},{"cell_type":"code","source":"#import libraries\nimport pandas as pd\nimport numpy as np\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import accuracy_score","metadata":{"execution":{"iopub.status.busy":"2024-06-10T09:16:09.034344Z","iopub.execute_input":"2024-06-10T09:16:09.035549Z","iopub.status.idle":"2024-06-10T09:16:09.737366Z","shell.execute_reply.started":"2024-06-10T09:16:09.035500Z","shell.execute_reply":"2024-06-10T09:16:09.736117Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## First review to data <a id=\"1.2\"></a>","metadata":{}},{"cell_type":"code","source":"#reading data\ndf1=pd.read_csv(\"/kaggle/input/creditcardfraud/creditcard.csv\")","metadata":{"execution":{"iopub.status.busy":"2024-06-10T09:16:09.738767Z","iopub.execute_input":"2024-06-10T09:16:09.739141Z","iopub.status.idle":"2024-06-10T09:16:13.537888Z","shell.execute_reply.started":"2024-06-10T09:16:09.739109Z","shell.execute_reply":"2024-06-10T09:16:13.536342Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df1\n#the Time columsn indicate that transaction completed in how many second\n# we can say some transaction occured in 1 second and  also last some transactions occured in 172786 seconds which is 47 hours","metadata":{"execution":{"iopub.status.busy":"2024-06-10T09:16:13.539549Z","iopub.execute_input":"2024-06-10T09:16:13.540079Z","iopub.status.idle":"2024-06-10T09:16:13.669405Z","shell.execute_reply.started":"2024-06-10T09:16:13.540037Z","shell.execute_reply":"2024-06-10T09:16:13.668151Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## NAN values  <a id=\"1.3\"></a>","metadata":{}},{"cell_type":"code","source":"# checking null values\ndf1.isnull().sum()","metadata":{"execution":{"iopub.status.busy":"2024-06-10T09:17:53.059210Z","iopub.execute_input":"2024-06-10T09:17:53.060166Z","iopub.status.idle":"2024-06-10T09:17:53.086532Z","shell.execute_reply.started":"2024-06-10T09:17:53.060124Z","shell.execute_reply":"2024-06-10T09:17:53.085117Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Distribution of Legit-Fraud classes <a id=\"1.4\"></a>","metadata":{}},{"cell_type":"code","source":"# checking distribution of Class column\ndf1[\"Class\"].value_counts()\n# as we can see  our data is highly unbalanced because difference between class object more than %99 . And we can't feed our ML model like that.","metadata":{"execution":{"iopub.status.busy":"2024-06-10T09:16:13.698595Z","iopub.execute_input":"2024-06-10T09:16:13.699037Z","iopub.status.idle":"2024-06-10T09:16:13.716110Z","shell.execute_reply.started":"2024-06-10T09:16:13.698998Z","shell.execute_reply":"2024-06-10T09:16:13.714867Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#  Separating datasets for analysis\nlegit=df1[df1.Class==0]\nfraud=df1[df1.Class==1]","metadata":{"execution":{"iopub.status.busy":"2024-06-10T09:16:13.717759Z","iopub.execute_input":"2024-06-10T09:16:13.718553Z","iopub.status.idle":"2024-06-10T09:16:13.762777Z","shell.execute_reply.started":"2024-06-10T09:16:13.718511Z","shell.execute_reply":"2024-06-10T09:16:13.761509Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Describing Amount column\nlegit.Amount.describe()","metadata":{"execution":{"iopub.status.busy":"2024-06-10T09:16:13.764301Z","iopub.execute_input":"2024-06-10T09:16:13.764666Z","iopub.status.idle":"2024-06-10T09:16:13.788838Z","shell.execute_reply.started":"2024-06-10T09:16:13.764637Z","shell.execute_reply":"2024-06-10T09:16:13.787796Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"fraud.Amount.describe()","metadata":{"execution":{"iopub.status.busy":"2024-06-10T09:16:13.790421Z","iopub.execute_input":"2024-06-10T09:16:13.790762Z","iopub.status.idle":"2024-06-10T09:16:13.803701Z","shell.execute_reply.started":"2024-06-10T09:16:13.790734Z","shell.execute_reply":"2024-06-10T09:16:13.802359Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# comparing Classe's averages\ndf1.groupby(\"Class\").mean()\n","metadata":{"execution":{"iopub.status.busy":"2024-06-10T09:16:13.805386Z","iopub.execute_input":"2024-06-10T09:16:13.806291Z","iopub.status.idle":"2024-06-10T09:16:13.942130Z","shell.execute_reply.started":"2024-06-10T09:16:13.806227Z","shell.execute_reply":"2024-06-10T09:16:13.940503Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Modelling  <a id=\"2\"></a>","metadata":{}},{"cell_type":"markdown","source":"## Under-Sampling for Unbalanced dataset  <a id=\"2.1\"></a>","metadata":{}},{"cell_type":"code","source":"# we'll use Under-Sampling method for handling with imbalancy\nlegit_sample=legit.sample(n=492) # n=492 is ","metadata":{"execution":{"iopub.status.busy":"2024-06-10T09:16:13.943754Z","iopub.execute_input":"2024-06-10T09:16:13.944102Z","iopub.status.idle":"2024-06-10T09:16:13.959802Z","shell.execute_reply.started":"2024-06-10T09:16:13.944071Z","shell.execute_reply":"2024-06-10T09:16:13.958649Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#creating new dataset after undersampling legit transaction\ndf2=pd.concat([legit_sample,fraud],axis=0)","metadata":{"execution":{"iopub.status.busy":"2024-06-10T09:16:13.961467Z","iopub.execute_input":"2024-06-10T09:16:13.961970Z","iopub.status.idle":"2024-06-10T09:16:13.971942Z","shell.execute_reply.started":"2024-06-10T09:16:13.961938Z","shell.execute_reply":"2024-06-10T09:16:13.970694Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#df2 is balanced new dataset\ndf2","metadata":{"execution":{"iopub.status.busy":"2024-06-10T09:16:13.973498Z","iopub.execute_input":"2024-06-10T09:16:13.973916Z","iopub.status.idle":"2024-06-10T09:16:14.013967Z","shell.execute_reply.started":"2024-06-10T09:16:13.973876Z","shell.execute_reply":"2024-06-10T09:16:14.012863Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Creating dependent-independent variables  <a id=\"2.2\"></a>","metadata":{}},{"cell_type":"code","source":"X,y=df2.drop(\"Class\",axis=1),df2[\"Class\"]","metadata":{"execution":{"iopub.status.busy":"2024-06-10T09:16:14.018272Z","iopub.execute_input":"2024-06-10T09:16:14.018680Z","iopub.status.idle":"2024-06-10T09:16:14.025127Z","shell.execute_reply.started":"2024-06-10T09:16:14.018647Z","shell.execute_reply":"2024-06-10T09:16:14.023972Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Splitting Test-Train Dataset  <a id=\"2.3\"></a>","metadata":{}},{"cell_type":"code","source":"#Train-Test splitting\nX_train, X_test, y_train, y_test = train_test_split(X,y,test_size=0.2,random_state=10,stratify=y)","metadata":{"execution":{"iopub.status.busy":"2024-06-10T09:16:14.026684Z","iopub.execute_input":"2024-06-10T09:16:14.027050Z","iopub.status.idle":"2024-06-10T09:16:14.039535Z","shell.execute_reply.started":"2024-06-10T09:16:14.027019Z","shell.execute_reply":"2024-06-10T09:16:14.038332Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Model Instance and Fitting <a id=\"2.4\"></a>","metadata":{}},{"cell_type":"code","source":"# Model Training\n# Logistic Regression\nmodel=LogisticRegression()\nmodel.fit(X_train,y_train)","metadata":{"execution":{"iopub.status.busy":"2024-06-10T09:16:14.041674Z","iopub.execute_input":"2024-06-10T09:16:14.042152Z","iopub.status.idle":"2024-06-10T09:16:14.048111Z","shell.execute_reply.started":"2024-06-10T09:16:14.042114Z","shell.execute_reply":"2024-06-10T09:16:14.047082Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Model Evaluation <a id=\"3\"></a>","metadata":{}},{"cell_type":"markdown","source":"## Evaluating with Accuracy score <a id=\"3.1\"></a>","metadata":{}},{"cell_type":"code","source":"#accuracy score on training data\nX_train_predicted=model.predict(X_train)\nprint(accuracy_score(X_train_predicted,y_train))","metadata":{"execution":{"iopub.status.busy":"2024-06-10T09:16:14.104057Z","iopub.execute_input":"2024-06-10T09:16:14.105056Z","iopub.status.idle":"2024-06-10T09:16:14.120102Z","shell.execute_reply.started":"2024-06-10T09:16:14.105013Z","shell.execute_reply":"2024-06-10T09:16:14.118438Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#accuracy score on test data\nX_test_predicted=model.predict(X_test)\nprint(accuracy_score(X_test_predicted,y_test))","metadata":{"execution":{"iopub.status.busy":"2024-06-10T09:16:14.122714Z","iopub.execute_input":"2024-06-10T09:16:14.123441Z","iopub.status.idle":"2024-06-10T09:16:14.135583Z","shell.execute_reply.started":"2024-06-10T09:16:14.123374Z","shell.execute_reply":"2024-06-10T09:16:14.134313Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Evaluating with Classification Evaluation Metrics  <a id=\"3.2\"></a>","metadata":{}},{"cell_type":"code","source":"from sklearn.metrics import classification_report\n\nprint(classification_report(y_test,X_test_predicted))","metadata":{"execution":{"iopub.status.busy":"2024-06-10T09:16:14.137226Z","iopub.execute_input":"2024-06-10T09:16:14.137897Z","iopub.status.idle":"2024-06-10T09:16:14.159874Z","shell.execute_reply.started":"2024-06-10T09:16:14.137856Z","shell.execute_reply":"2024-06-10T09:16:14.158731Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Evaluating with ROC curve and AUC score <a id=\"3.3\"></a>","metadata":{}},{"cell_type":"code","source":"#getting prediction probablities from model\ny_test_prob=model.predict_proba(X_test)\ny_test_prob[:10] # those are the first 10 data's probablities. First column mean is probablity of fraud transaction.second columns is probablity of legit transaction(or reverse idk)","metadata":{"execution":{"iopub.status.busy":"2024-06-10T09:16:14.161558Z","iopub.execute_input":"2024-06-10T09:16:14.162232Z","iopub.status.idle":"2024-06-10T09:16:14.175414Z","shell.execute_reply.started":"2024-06-10T09:16:14.162192Z","shell.execute_reply":"2024-06-10T09:16:14.173999Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"y_test_prob_positve=y_test_prob[:,1] #getting second column in variable\n\n#calculating fpr,tpr and thresholds\nfrom sklearn.metrics import roc_curve\nfpr,tpr,thresholds=roc_curve(y_test,y_test_prob_positve)\n\n#check the false positive rate\nfpr","metadata":{"execution":{"iopub.status.busy":"2024-06-10T09:16:14.177362Z","iopub.execute_input":"2024-06-10T09:16:14.178415Z","iopub.status.idle":"2024-06-10T09:16:14.191626Z","shell.execute_reply.started":"2024-06-10T09:16:14.178370Z","shell.execute_reply":"2024-06-10T09:16:14.190385Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Plotting ROC Curve  <a id=\"3.3.1\"></a>","metadata":{}},{"cell_type":"code","source":"# Creating function for plotting ROC Curve\nimport matplotlib.pyplot as plt\n\n\ndef plot_roc_curve(fpr,tpr):\n    \n    \n    #plot roc curve\n    plt.plot(fpr,tpr,color=\"orange\",label=\"ROC\")\n    \n    # Plot line with no predictive power(baseline)\n    plt.plot([0,1],[0,1],color=\"darkblue\",linestyle=\"--\",label=\"Guessing\")\n    \n    # Customize the plot\n    plt.xlabel(\"False Positive Rate(fpr)\")\n    plt.ylabel(\"True Positive Rate(tpr)\")\n    plt.title(\"Reciever Operating Characteristics(ROC) Curve\")\n    \n    \n    \nplot_roc_curve(fpr,tpr)","metadata":{"execution":{"iopub.status.busy":"2024-06-10T09:16:14.193824Z","iopub.execute_input":"2024-06-10T09:16:14.194528Z","iopub.status.idle":"2024-06-10T09:16:14.573077Z","shell.execute_reply.started":"2024-06-10T09:16:14.194487Z","shell.execute_reply":"2024-06-10T09:16:14.571813Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Calculating AUC score  <a id=\"3.3.2\"></a>","metadata":{}},{"cell_type":"code","source":"from sklearn.metrics import roc_auc_score\nroc_auc_score(y_test,y_test_prob_positve)","metadata":{"execution":{"iopub.status.busy":"2024-06-10T09:16:14.574556Z","iopub.execute_input":"2024-06-10T09:16:14.574975Z","iopub.status.idle":"2024-06-10T09:16:14.587456Z","shell.execute_reply.started":"2024-06-10T09:16:14.574942Z","shell.execute_reply":"2024-06-10T09:16:14.586419Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Conclusion <a id=\"4\"></a>","metadata":{}},{"cell_type":"markdown","source":"So ı could say my model is predicting very well when we take into considaration auc score (auc score as close as 1 is better for classification model )","metadata":{}}]}